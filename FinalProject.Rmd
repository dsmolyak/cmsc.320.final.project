---
title: "Final Project: Analyzing U.S. Election County Data"
author: "Daniel Smolyak"
date: "5/16/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

With the recent U.S. presidential election behind us and and a midterm election in the near future, it is easy to focus on the anecdotal headlines and stories on individual voters, individual districts, or even individual states. However, a thorough data analysis of the characteristics of each and every county and how they voted in each election can provide us with a clearer and more accurate idea of just what drives the general trends in political leanings, regardless of these specific anecdotes.

Simultaneously, this will be a tutorial in data analysis using R. We will first examine how to collect raw data on these county characteristics and votes. Then, we will go through the processing of this data in order to prepare it for analysis. Then, we will visualize the data in order to conduct exploratory analysis. From there, we will focus in and analyze patterns we've discovered in the visualization. And from these patterns, we will draw conclusions on general trends within the U.S. electorate. And using these general trends, we hope to draw insights on why certain areas of the country lean more Democratic or Republican, in order to better understand the overall results of past and upcoming elections.

![](./politics.jpg)

# Setup

Before we start, you'll want to make sure you have the proper packages installed and imported as shown below. Some common packages in R include tidyverse, dplyr, and stringr, among others.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(stringr)
```

# Data Collection

The data used for this project can be found online at this [kaggle repository](https://www.kaggle.com/joelwilson/identity-politics-in-the-2016-election/data). At this repository, we find two csv files, one on each U.S. county's [demographic characteristics](https://www.kaggle.com/joelwilson/2012-2016-presidential-elections/downloads/county_facts.csv/2) and another on its [voting behavior](https://www.kaggle.com/joelwilson/2012-2016-presidential-elections/downloads/US_County_Level_Presidential_Results_12-16.csv/2).

So the first step in collecting this data is of course to download the datasets, by clicking on the link above. Then, after placing these csv files in the same folder as your analysis, we can execute the code below, which converts csv files into useable data in R, in the form of a data frame.


```{r message=FALSE, warning=FALSE}
path <- "./county_facts.csv"
county_data <- read_csv(path)
county_data
```

```{r message=FALSE, warning=FALSE}
path <- "./US_County_Level_Presidential_Results_12-16.csv"
election_data <- read_csv(path)
election_data
```

# Basic Data Manipulation

Now, we may explore these specific data frames in a variety of ways to get a general sense of what is contained within them. For example we might ask how many rows (counties) there are:


```{r message=FALSE, warning=FALSE}
nrow(county_data)
```

Or we might ask what are and how many columns (attributes) do we have:

```{r message=FALSE, warning=FALSE}
colnames(county_data)
```

And as the above column names may seem pretty archaic, we have thankfully also been provided with a dictionary that maps these names to their meanings.

```{r message=FALSE, warning=FALSE}
path <- "./county_facts_dictionary.csv"
county_dict <- read_csv(path)
county_dict
```

Now that we have a general understanding of what lies within our data frame, let's also introduce some methods for searching for specifics within the data. For example, we can choose to look at only certain attributes, using the "select" command.

```{r message=FALSE, warning=FALSE}
select(county_data, area_name, PST045214, SEX255214)
```

Or we can choose to look at certain rows (entities) that fit a specific description or condition. For instance, the code below returns us the row with our very own Prince George's County.

```{r message=FALSE, warning=FALSE}
filter(county_data, area_name == "Prince George's County")
```

We can even use the FIPS (Federal Information Processing Standard) code to find all of the counties recorded that are in Maryland, as we know that those every 1000 fips are for one state, and the result above indicates the Maryland FIPS are between 24000 and 25000.

```{r message=FALSE, warning=FALSE}
filter(county_data, fips >= 24000 & fips < 25000)
```

# Data Linkage

Now, to make this data useable for comparison of county demographics to county voting trends, we need to combine these two data frames. We will be doing this two-table operations, specifically by "joining" the demographics table and the voting table on their matching value for FIPS. Such operations are also frequently done in SQL, where information is separated in a database into tables. You can read more about these two table operations [here](https://www.w3schools.com/sql/sql_join.asp), but we will show our specific use-case below.

```{r message=FALSE, warning=FALSE}
total_df <- county_data %>%
  inner_join(election_data, by = c("fips" = "combined_fips"))
total_df
```

# Vizualization

Now our data is all together in one data frame, we may begin analyzing some basic trends and patterns in the data. For instance, we can plot in a histogram the number of counties in various ranges of political leanings. We will do this by using the "per_point_diff_2016" statistic, which is the percentage differential between Democratic and Republican votes in 2016. (subtracts the Republican percentage from the Democratic percentage). Thus, counties leaning Republican will be negative while those leaning Democratic will be positive. Additionally, we include two lines to indicate where the median and mean leaning is of a county in 2016.

We do this using the ggplot package, which allows to specify what type of plot we'd like (histogram), what we'd like as the x-variable (per_point_diff_2016) and the other vertical that are plotted below, using geom_vline.

```{r message=FALSE, warning=FALSE}
total_df %>%
  ggplot(aes(x=per_point_diff_2016)) +
      geom_histogram(bins=100) +
      geom_vline(aes(xintercept=median(per_point_diff_2016)), color="red") +
      geom_vline(aes(xintercept=mean(per_point_diff_2016)), color="blue")
```

From the visualization above, it might seem like the typical voter in the U.S. leans heavily Republican, but all this visualization literally says is that the typical county leans Republican. And upon further examination, we would likely see that weighting counties by population would likely return the mean (as opposed to the median) to much closer to 0, as the highest population counties occur least often and are most often heavily Democratic. 

Thus, below we calculate this "weighted lean" by simply finding the difference between the number of Democratic and Republican votes. And below we can see that the mean, which appropriately weights by population produces a slightly Democratic lean, as the popular vote in 2016 did lean Democratic, while the median, which goes purely by number of counties, leans much more heavily Republican.

```{r message=FALSE, warning=FALSE}
weight_df <- total_df %>% mutate(weighted_lean = votes_dem_2016 - votes_gop_2016)
summarize(weight_df, mean_lean = mean(weighted_lean), median_lean = median(weighted_lean))
```

Furthermore, we can plot these weights on a new histogram, as shown below.

```{r message=FALSE, warning=FALSE}
weight_df %>%
  ggplot(aes(x=weighted_lean)) +
      geom_histogram(bins=100) +
      geom_vline(aes(xintercept=mean(weighted_lean)), color="blue") +
      geom_vline(aes(xintercept=median(weighted_lean)), color="red")
```

However, it is immediately visible that these weights are too far spread out to get a good feel for the data, so we now instead transform the data, using the log function. Specifically we take the log of positive values, and for negative values, we take their absolute value, then take their log, and then multiply by -1. 
This allows us a cleaner view of the data: we can now see the substantial difference between the mean and median. Additionally, we can see the trend suspected before, that while there are fewer Democratic leaning counties, they tend to be the most populous counties.

```{r message=FALSE, warning=FALSE}
weight_df %>%
  mutate(log_weight = ifelse(weighted_lean > 0, log(weighted_lean), log(abs(weighted_lean)) * -1)) %>%
  ggplot(aes(x=log_weight)) +
      geom_histogram(bins=100) +
      geom_vline(aes(xintercept=log(mean(weighted_lean))), color="blue") +
      geom_vline(aes(xintercept= (log(abs(median(weighted_lean)))) * -1), color="red")
```

# Hypothesis Testing

Next, since we have our unified data set, let's test out what effects certain county characteristics have on political leaning. For instance, it's often been said in the news that Donald Trump has pulled more educated voters to the left. Thus, in formal terms, we will hypothesize that an increase in the percentage of residents having a bachelor's degree will lead to an increase in the percent differential in Democratic vs. Republican votes.

Let's start by plotting these two variables against one another:

```{r message=FALSE, warning=FALSE}
total_df %>%
  ggplot(aes(x = EDU685213, y = per_point_diff_2016)) +
      geom_point()
```

From this visualization it is clear that a trend might exist, but attempt to formalize that trend. We will specifically attempt to disprove the null-hypothesis, which would be the lack of trend between these two variables. The way we will do this is by plotting a regression line: if the line goes diagonally to the upper right, there may be a trend. So let's see:

```{r message=FALSE, warning=FALSE}
total_df %>%
  ggplot(aes(x = EDU685213, y = per_point_diff_2016)) +
      geom_point() +
      geom_smooth(method = lm)
```

Awesome! We clearly see this regression line indicates an increase in education leads to an increase in Democratic leaning. Now let's try and figure out exactly what this regression line means. Thus, we will create a linear model as shown below, determining the differential as a function of education level.

```{r message=FALSE, warning=FALSE}
education_model <- lm(total_df$per_point_diff_2016 ~ total_df$EDU685213)
broom::tidy(education_model)
```

This model now gives us information on two fronts. Firstly, the estimate for "total_df$EDU685213" gives us an understanding of what exactly the relation is between education and differential. Because the model states an estimate of approximately 0.016, we can interpret this to mean that for every 1% increase in education level in any given county, we can expect a 0.016 (1.6%) increase in the differential between Democratic and Republican votes. Additionally, the intercept tells us that for a county with 0% of its residents with bachelor's degrees, the expected differential in that county would be approximately -0.635, or a strong Republican leaning.

Moreover, if we look at the p-value for both the intercept and the relationship, they are both nearly 0. Thus, as the p-value is certainly below the standard threshold of 0.05, with strong confidence, we can reject our null-hypothesis of no trend between education and political leaning.

The insight: while this regression provides no indication whatsoever of how well-informed policies on either side of the aisle are, it certainly appears that Republicans do a better job of appealing to voters of lower education levels and Democrats to voters of higher education levels. To read more on this trend you can visit this [Pew Research article](http://www.pewresearch.org/fact-tank/2016/09/15/educational-divide-in-vote-preferences-on-track-to-be-wider-than-in-recent-elections/).

# Machine Learning

Last but not least, we will use machine learning to make a model that incorporates several of these county characteristics that are likely to factor into the outcome of an election. We will stick to four main indicators:
1. Percent of people with a Bachelor's degree (EDU685213): As we previously showed in our hypothesis testing, there is a clear link between higher levels of education and increased Democratic lean
2. Percent of people that are White (RHI125214): While this is more speculation at this point, it is commonly known that minorities tend to vote Democratic while White's tend to vote Republican, we believe this will be an important indicator
3. Median Household Income (INC110213): Once again, median income, especially in the 2016 election was generally thought to be correlated with greater Democratic lean.
4. Population per square mile, aka population density (POP060210): With this metric, we hope to target cities versus rural areas, where cities generally lean Republican.

Now we shall start by converting election lean into a factor (D or R).

```{r message=FALSE, warning=FALSE}
outcome_df <- total_df %>%
  mutate(result = ifelse(per_point_diff_2016 > 0, 'D', 'R')) %>%
  select(fips, result)
outcome_df
```

And then we select these four indicators, along with the outcome (which we convert to a factor).

```{r message=FALSE, warning=FALSE}
final_df <- total_df %>%
  mutate(result = ifelse(per_point_diff_2016 > 0, 'D', 'R')) %>%
  mutate(result=factor(result, levels=c("D", "R"))) %>%
  select(EDU685213, RHI125214, INC110213, POP060210, fips, result)

final_df
```

Now, we partition this data into training and testing sets: we use the training set to train our model and then evaluate the performance of our model on the testing set.

```{r message=FALSE, warning=FALSE}
set.seed(1234)
test_random_forest_df <- final_df %>%
  group_by(result) %>%
  sample_frac(.2) %>%
  ungroup()

train_random_forest_df <- final_df %>%
  anti_join(test_random_forest_df, by="fips")
```

Now that we've partitioned our data, we will train a random forest (more information about these [here](https://en.wikipedia.org/wiki/Random_forest)) on our training set.

```{r message=FALSE, warning=FALSE}
library(randomForest)
rf <- randomForest(result~., data=train_random_forest_df %>% select(-fips))
rf
```

Already we see that while this model does a good job of identifying Republican counties (as most counties are Republican), it often struggles with classifying Democratic counties, having a 40% error rate of that type. However, overall the model is estimated to have an 8.8% error rate, which is a decent start. Now let's see how well we do on our training data:

```{r message=FALSE, warning=FALSE}
test_predictions <- predict(rf, newdata=test_random_forest_df %>% select(-fips))
table(pred=test_predictions, observed=test_random_forest_df$result)
```

Thus, in this particular test, we observe an error rate of (15 + 28)/(15 + 28 + 70 + 516) = .0684 or 6.84%. While this certainly isn't perfect, this shows that our four chosen indicators do a decent job, and certainly better than random job, of predicting the outcome of an election in any given county. There is of course much opportunity for improvement here, such as using different models, making transformations of the data, or using different indicators.

However, the main point is that with R and its various libraries, and with a proper dataset at hand, we can make great leaps in bounds in understanding the world around us more concretely. In particular here, we went beyond the individual stories of Hillary Clinton and Donald Trump, or of the mythical swing voter, and looked at specific trends within our electorate in the 2016 election. And with these data science tools, we can confirm or deny various reports or stories that we might see, making us better consumers of news and information.

Hope you enjoyed! And here are some of the resources on the packages we used:
[Dplyr](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html)
[Tidyverse](https://www.tidyverse.org/)
[Randomforest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf)



